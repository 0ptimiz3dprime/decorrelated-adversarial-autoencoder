# decorrelated-adversarial-autoencoder
Tensorflow implementation of Adversarial Autoencoders (with extra option to decorrelate style and classes)

## How it works

This is a semi-supervised adversarial autoencoder as described in the original paper by Allreza Makhzani et al ([arxiv.org/abs/1511.05644](https://arxiv.org/abs/1511.05644)) with the following modification:

I added the possibility to de-correlate the style and class to allow for nice sampling of the style space, and because without this, q(y|X) could be predicted with 60-80% accuracy from q(z|X), meaning that they were heavily correlated and that q(z|X) did not only contain information about the style, but also about the digit in the image, effectively making class-information surpass q(y|X). Adding de-correlation results in that the classifying part of the network only achieve 10% accuracy in predicting q(y|X) from q(z|X), which can be interpreted as there being no correlation at all for the 10 MNIST classes.

![Network architecture](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/decorrelator-network-01.png)

(modified version of Figure 8 from the original paper)

# Examples

The following examples were generated by a network trained in a semi-supervised fashion on just 10 labelled examples of each of the MNIST-classes, as well as all of the unlabelled training data. The hyperparameters for this training were as in the available source code.

## Capturing the style of real examples

Using existing MNIST examples, **X**, one can feed these through to capture their style q(**z** | **X**) and generate all the possible digits from 0 to 9 in the same style.

The first column below contains samples from the MNIST dataset, and each row next to these samples is the network's interpretation of their style applied to the 10 digits:

![Capturing of the style of MNIST digits](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/captured_digit_style.png)

## Sampling the style space

As **z**, the style space, is an N-dimensional space with values independently distributed as Gaussian distributions with mean 0, and variance 1, this can be sampled and the digits from 0 to 9 can be generated in the same style:

![Randomly styled digits, no. 1](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_0.png)

![Randomly styled digits, no. 2](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_1.png)

![Randomly styled digits, no. 3](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_2.png)

## Style-interpolation.

This can be done by sampling two points in **z** and interpolating linearly (or otherwise) between them:

![Interpolation between random styles, no. 1](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_interpolation_0.png)

![Interpolation between random styles, no. 2](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_interpolation_1.png)

![Interpolation between random styles, no. 3](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_interpolation_2.png)

![Interpolation between random styles, no. 4](https://raw.githubusercontent.com/patrickgadd/decorrelated-adversarial-autoencoder/master/assets/digit_style_interpolation_3.png)